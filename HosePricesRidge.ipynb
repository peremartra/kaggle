{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/peremartramanonellas/notebook-with-explanations-deal-with-the-data?scriptVersionId=105293144\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"# Introduction\n\nAs you know this is based in the competition **House Prices - Advanced Regression Techniques** \n\nWhat can be found in the notebook? \n- Identify columns with few values that we can drop. \n- How to treat the NULL values. \n- Study the data with BoxPlots, scatterplots, heatmaps... \n- Identify the most correlated features.\n- Identify and correct the skew values. \n- Transform all the data to numeric. \n- Categorize the data. \n- Scale the numeric data, if necesary, with StandarScaler or MinMaxScaler. \n- Use a model, get predictions and submit results. \n\nI did my best to explain each step. If you have any comment or question, please don't hesitate to use the comments section. \n\n- <a href='#sectiondata'>Data</a>\n\n  - <a href='#cleannulls'>Clean Nulls</a>\n\n    - <a href='#heatmap'>HeatMap Garage</a>\n\n  - <a href='#scaling'>Scaling</a>\n\n  - <a href='#correlated'>Select correlated features</a>\n\n- <a href='#training'>Create & Training the model</a>\n\n  - <a href='#submission'>Submission</a>\n \n- <a href='#inspirations'>Inspirations</a>\n\n","metadata":{}},{"cell_type":"code","source":"#Import libraries \n\nimport os \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\nfrom scipy.stats import skew, norm\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\n%matplotlib inline   ","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:02.195097Z","iopub.execute_input":"2022-09-08T20:12:02.196479Z","iopub.status.idle":"2022-09-08T20:12:03.196064Z","shell.execute_reply.started":"2022-09-08T20:12:02.196324Z","shell.execute_reply":"2022-09-08T20:12:03.194676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ndf_test = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\n\n#I'm going to join the train & Test Data in order to do the same transformations. \ndf_housing=pd.concat([df_train, df_test], axis=0).reset_index(drop=True)\n\n#keeping the number of elements in the train dataframe. \nTRAIN_ELEMENTS_INDEX = df_train.index.max()\n\ndf_train.shape, df_test.shape, df_housing.shape\n","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:03.198341Z","iopub.execute_input":"2022-09-08T20:12:03.198821Z","iopub.status.idle":"2022-09-08T20:12:03.335912Z","shell.execute_reply.started":"2022-09-08T20:12:03.19878Z","shell.execute_reply":"2022-09-08T20:12:03.334828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data.<a id='sectiondata'></a> \nActions that we should do with the Data: \n- Identify columns with unique values.  \n- Clean / Remove / Replace the Nulls. \n- Replace all the non numeric values\n- Check for outliers\n- Standarize / Normalize or adapt the numeric values.\n\n ","metadata":{}},{"cell_type":"code","source":"#Just a quick look to our data. \ndf_housing.head(20)","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:03.337833Z","iopub.execute_input":"2022-09-08T20:12:03.338695Z","iopub.status.idle":"2022-09-08T20:12:03.393282Z","shell.execute_reply.started":"2022-09-08T20:12:03.33865Z","shell.execute_reply":"2022-09-08T20:12:03.392403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, we have some columns with just a few values, let's check their *uniquevalues* and take a decisi贸n if we want to keep, or not, the column. ","metadata":{}},{"cell_type":"code","source":"#check the number of elements in each unique value in the columns. c\nCOLS_TO_CHECK = ['Street', 'Utilities', 'Condition2', \n                'LandSlope', 'RoofMatl', 'ExterCond', 'Heating', \n                'CentralAir', 'GarageCond', 'Electrical']\n\n#I create a DataFrame where store the Data\ndftemp = pd.DataFrame(columns=['col_name', 'value', 'count'])\n\n#In every colum to check I get the values and number of them, \n#and store in the DataFrame\nfor ctc in COLS_TO_CHECK: \n    dcol = df_housing[ctc].value_counts()\n    for i in range(len(dcol)):\n        row = {'col_name': ctc, \n              'value': dcol.index[i], \n              'count': dcol[i]}\n        df_row = pd.DataFrame([row])\n        dftemp = pd.concat([dftemp, df_row], axis=0, ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:03.396407Z","iopub.execute_input":"2022-09-08T20:12:03.397206Z","iopub.status.idle":"2022-09-08T20:12:03.495243Z","shell.execute_reply.started":"2022-09-08T20:12:03.397158Z","shell.execute_reply":"2022-09-08T20:12:03.493816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#A list of all the values, it's easy to identify some that we can drop. \ndftemp","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:03.497266Z","iopub.execute_input":"2022-09-08T20:12:03.497834Z","iopub.status.idle":"2022-09-08T20:12:03.516002Z","shell.execute_reply.started":"2022-09-08T20:12:03.497782Z","shell.execute_reply":"2022-09-08T20:12:03.51468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's clear to me that we can Drop *Street* and *Utilities* without any doubt. In *Utilities* we have just two values and one of them is present in only one row. ","metadata":{}},{"cell_type":"code","source":"df_housing.drop(['Street', 'Utilities'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:03.51754Z","iopub.execute_input":"2022-09-08T20:12:03.51855Z","iopub.status.idle":"2022-09-08T20:12:03.531021Z","shell.execute_reply.started":"2022-09-08T20:12:03.518499Z","shell.execute_reply":"2022-09-08T20:12:03.529832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Clean the Nulls.<a id='cleannulls'></a> ","metadata":{}},{"cell_type":"code","source":"#change the ppandas visualization options to view all columns an rows if necessary. \npd.set_option('display.max_rows', 1000); pd.set_option('display.max_columns', 1000); \ndf_housing.shape","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:03.532611Z","iopub.execute_input":"2022-09-08T20:12:03.533051Z","iopub.status.idle":"2022-09-08T20:12:03.551137Z","shell.execute_reply.started":"2022-09-08T20:12:03.533017Z","shell.execute_reply":"2022-09-08T20:12:03.54966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have 2919 rows by 79 columns in the train Dataset . ","metadata":{}},{"cell_type":"code","source":"df_housing.head(5)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-09-08T20:12:03.553786Z","iopub.execute_input":"2022-09-08T20:12:03.556944Z","iopub.status.idle":"2022-09-08T20:12:03.626381Z","shell.execute_reply.started":"2022-09-08T20:12:03.556899Z","shell.execute_reply":"2022-09-08T20:12:03.625265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Get the numbers of Null values in columns, in descending order. \ndf_housing.isna().sum().sort_values(ascending=False).head(40\n                                                         )","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:03.628053Z","iopub.execute_input":"2022-09-08T20:12:03.628802Z","iopub.status.idle":"2022-09-08T20:12:03.6534Z","shell.execute_reply.started":"2022-09-08T20:12:03.628753Z","shell.execute_reply":"2022-09-08T20:12:03.652528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are columns with a lot of empty rows. Usually **the best that we can do with the columns with a big number of empty data is delete it**, because we can't fill with invented values. \n\nTo deal with empty data we have some different options how to deal with empty values: \n\n**Delete the column:** When the number of rows with empty values is really big, the best option is delete the entire column. I have no a magic number, but if it's more than 80% of rows with empty values I normally delete the column. \n\n**Replace the values:** Sometimes there are empty values because it indicates a category, and this category dosn't apply to some of the registers. \nThat's the case of Fence in this dataset. There are a lot of properties without a Fence, but the properties with a Fence have different categories of Fences that affect to the price of this properties. In this case we can fill the empty row with the category *No. \n\n**Delete the rows**: When the empty is in a feature with a strong correlation with the label, the best solution is delete the rows. But in this case I'm going to avoid it, because I Joined the train and test dataset. To be able to delete rowns I should have joined them with a different technique. \n\n**By the moment, we are going to delete the columns:PoolQC. MiscFeature,  Alley and Id.**\n","metadata":{}},{"cell_type":"code","source":"df_house_clean = df_housing.drop(['Id', 'PoolQC', 'MiscFeature', 'Alley'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:03.657535Z","iopub.execute_input":"2022-09-08T20:12:03.658203Z","iopub.status.idle":"2022-09-08T20:12:03.666474Z","shell.execute_reply.started":"2022-09-08T20:12:03.658165Z","shell.execute_reply":"2022-09-08T20:12:03.665279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.title('Fence ')\n#Note that I'm getting only the Train part of the Dataframe, the test have no SalePrice\nsns.boxplot(x='Fence', y='SalePrice', data=df_house_clean[:TRAIN_ELEMENTS_INDEX:])","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-09-08T20:12:03.668145Z","iopub.execute_input":"2022-09-08T20:12:03.669195Z","iopub.status.idle":"2022-09-08T20:12:03.983084Z","shell.execute_reply.started":"2022-09-08T20:12:03.669158Z","shell.execute_reply":"2022-09-08T20:12:03.9821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I can't se a clear relathionship between Fence and the SalePrice, and there are a lot of outliers values. We can just delete this value too. ","metadata":{}},{"cell_type":"code","source":"df_house_clean.drop(['Fence'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:03.984716Z","iopub.execute_input":"2022-09-08T20:12:03.98575Z","iopub.status.idle":"2022-09-08T20:12:03.995708Z","shell.execute_reply.started":"2022-09-08T20:12:03.985702Z","shell.execute_reply":"2022-09-08T20:12:03.994156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Check the unique values in FirePlaceQu\ndf_house_clean['FireplaceQu'].unique()","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:03.997488Z","iopub.execute_input":"2022-09-08T20:12:03.999001Z","iopub.status.idle":"2022-09-08T20:12:04.008722Z","shell.execute_reply.started":"2022-09-08T20:12:03.998779Z","shell.execute_reply":"2022-09-08T20:12:04.007276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I'm going to fill the Null Values with a \"No\", just to have more information in the subplot","metadata":{}},{"cell_type":"code","source":"df_house_clean['FireplaceQu'] = df_house_clean['FireplaceQu'].fillna('No')\nplt.title('FireplaceQu ')\nsns.boxplot(x='FireplaceQu', y='SalePrice', data=df_house_clean[:TRAIN_ELEMENTS_INDEX:])","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:04.010345Z","iopub.execute_input":"2022-09-08T20:12:04.0115Z","iopub.status.idle":"2022-09-08T20:12:04.326104Z","shell.execute_reply.started":"2022-09-08T20:12:04.011452Z","shell.execute_reply":"2022-09-08T20:12:04.325195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ok, it seems that it can be a correlation between the type of fireplace and the *SalePrice*, but it's not strong enough and there are a lot of outliners. I'm not going to keep it. ","metadata":{}},{"cell_type":"markdown","source":"Time to check the values in *LotFrontage*. It contains the linear feet of street connected to property. As a resident in a City in Europe I have no Idea if this data can be important or not in the local Market of the Dataset. By the moment we can fill the values with the mean. ","metadata":{}},{"cell_type":"code","source":"df_house_clean['LotFrontage'].unique()","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:04.327396Z","iopub.execute_input":"2022-09-08T20:12:04.328606Z","iopub.status.idle":"2022-09-08T20:12:04.339784Z","shell.execute_reply.started":"2022-09-08T20:12:04.328538Z","shell.execute_reply":"2022-09-08T20:12:04.3383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.title('LotFrontage ')\nsns.scatterplot(x='LotFrontage', y='SalePrice', data=df_house_clean[:TRAIN_ELEMENTS_INDEX:])","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:04.341945Z","iopub.execute_input":"2022-09-08T20:12:04.342444Z","iopub.status.idle":"2022-09-08T20:12:04.611153Z","shell.execute_reply.started":"2022-09-08T20:12:04.342399Z","shell.execute_reply":"2022-09-08T20:12:04.609987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I cant see a clear relation between the *LotFrontage* and the *SalePrice*, I'm  going to delete this value too. \n","metadata":{}},{"cell_type":"code","source":"df_house_clean.drop(['LotFrontage'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:04.612607Z","iopub.execute_input":"2022-09-08T20:12:04.613717Z","iopub.status.idle":"2022-09-08T20:12:04.622988Z","shell.execute_reply.started":"2022-09-08T20:12:04.613667Z","shell.execute_reply":"2022-09-08T20:12:04.62198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check again the columns with empty values\ndf_house_clean.isna().sum().sort_values(ascending=False).head(15)","metadata":{}},{"cell_type":"code","source":"df_house_clean.isna().sum().sort_values(ascending=False).head(15)","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:04.6247Z","iopub.execute_input":"2022-09-08T20:12:04.626066Z","iopub.status.idle":"2022-09-08T20:12:04.644942Z","shell.execute_reply.started":"2022-09-08T20:12:04.62602Z","shell.execute_reply":"2022-09-08T20:12:04.643571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We still have a lot of columns with just a few Null values. But we need to decide what to do with them. It's important to know if the data is just numeric or it can be categorical. And when we know it, fill it, or maybe just remove the rows, or columns. \n\nWe have 5 Garage variables with 81 registers with Nulls, but we have others columns related to the garage without missing data. To decide if we can Delete the columns, the rows or fill it. We can study the correlation of all the garage columns, and try to identify wich one is more important to our model. \n\nAt this moment some of this columns are not numeric, and we can calculate the Correlation. We can wait until all our Data is numeric, but I prefer to identify now if we can delete some of this data. \n","metadata":{}},{"cell_type":"markdown","source":"### Heatmap Garage variables<a id='heatmap'></a>","metadata":{}},{"cell_type":"code","source":"#saleprice correlation matrix with Garage Variables. \ncols = ['SalePrice','GarageType', 'GarageYrBlt', 'GarageCond', \n       'GarageFinish', 'GarageQual', 'GarageCars', 'GarageArea']\n\ncorrmat = df_house_clean[:TRAIN_ELEMENTS_INDEX:][cols].corr()\n\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, annot=True, cmap=\"Blues\");","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-09-08T20:12:04.646711Z","iopub.execute_input":"2022-09-08T20:12:04.647452Z","iopub.status.idle":"2022-09-08T20:12:05.011434Z","shell.execute_reply.started":"2022-09-08T20:12:04.647405Z","shell.execute_reply":"2022-09-08T20:12:05.01019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By the moment we can delete *GarageYrBlt*, and keep *GarageArea* and *GarageCars*, but for the name for sure that this Data is redundant. And if I have to choose one, I will choose *GarageArea*, but in the HeatMap we can see that *GarageCars* is more correlated, so....I'm going to Keep *GarageCars* (I believe in numbers). Maybe some garages have a large Area but it's not possible to fit more than one car. ","metadata":{}},{"cell_type":"code","source":"#Delete the columns\ndf_house_clean.drop(['GarageYrBlt', 'GarageArea'], axis=1, inplace=True)\n\n\n#Fill the column\ndf_house_clean = df_house_clean.fillna(df_house_clean.mode().iloc[0])\n","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:05.013324Z","iopub.execute_input":"2022-09-08T20:12:05.014159Z","iopub.status.idle":"2022-09-08T20:12:05.086067Z","shell.execute_reply.started":"2022-09-08T20:12:05.014112Z","shell.execute_reply":"2022-09-08T20:12:05.085144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check if we have more Nulls: ","metadata":{}},{"cell_type":"code","source":"df_house_clean.isna().sum().sort_values(ascending=False).head(15)","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:05.087516Z","iopub.execute_input":"2022-09-08T20:12:05.088112Z","iopub.status.idle":"2022-09-08T20:12:05.103648Z","shell.execute_reply.started":"2022-09-08T20:12:05.088077Z","shell.execute_reply":"2022-09-08T20:12:05.102239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Perfect, **we have no Nulls**. ","metadata":{}},{"cell_type":"markdown","source":"## Check the skew of the numeric values. ","metadata":{}},{"cell_type":"code","source":"\n#obtain all the numeric columns \nnumeric_columns = list(df_house_clean.select_dtypes(['int16', 'int32', 'int64', 'float16', 'float32', 'float64']).columns)\nnumeric_columns\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:05.105205Z","iopub.execute_input":"2022-09-08T20:12:05.106399Z","iopub.status.idle":"2022-09-08T20:12:05.118582Z","shell.execute_reply.started":"2022-09-08T20:12:05.106351Z","shell.execute_reply":"2022-09-08T20:12:05.117352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skew_features = df_house_clean.select_dtypes(['int16', 'int32', 'int64', 'float16', 'float32', 'float64']).apply(lambda x: skew(x)).sort_values(ascending=False)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:05.120429Z","iopub.execute_input":"2022-09-08T20:12:05.121259Z","iopub.status.idle":"2022-09-08T20:12:05.145168Z","shell.execute_reply.started":"2022-09-08T20:12:05.121204Z","shell.execute_reply":"2022-09-08T20:12:05.143889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"high_skew = skew_features[skew_features > 0.5]\n\n#in skew_index we store the name of the columns\nskew_index = high_skew.index\n\nprint(\"There are {} numerical features with Skew > 0.5 :\".format(high_skew.shape[0]))\n\n#Store the skews colums in a DataFrame\nskewness = pd.DataFrame({'Skew' :high_skew})\nskew_features.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:05.147042Z","iopub.execute_input":"2022-09-08T20:12:05.147787Z","iopub.status.idle":"2022-09-08T20:12:05.162232Z","shell.execute_reply.started":"2022-09-08T20:12:05.147734Z","shell.execute_reply":"2022-09-08T20:12:05.160987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in skew_index: \n    if (col != 'SalePrice'):\n        df_house_clean[col] = boxcox1p(df_house_clean[col], \n                                   boxcox_normmax(df_house_clean[col]+1))\n    ","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:05.164248Z","iopub.execute_input":"2022-09-08T20:12:05.165067Z","iopub.status.idle":"2022-09-08T20:12:05.393191Z","shell.execute_reply.started":"2022-09-08T20:12:05.165018Z","shell.execute_reply":"2022-09-08T20:12:05.39164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Transform Data to numeric. \nAll the data must be numeric. And we have some columns that are in text. \n\nWe have two ways to convert datas in numeric. \n- In the same column. We can assign a number to each category. But it have a problem. If we assign the numbers 1, 2 and 3 to three diferent categories. Our model will be confused because 2 is bigger than 1 and 3 bigger than 2, but it isn't important, we are just categorizing. I only recommend this kind of categorization in fields where bigger values have more impact in the label than the smaller ones. If we have a Column called GarageSize with the values small, medium, and big. We can convert this categories into 1, 2 and 3 (or a normalized value), because the size of the garage have a positive correlation with the prices of property. \n\n- In Categories. We create a new column for each value, and indicate if the row have this value with an 1 or a 0. This the most commom way to transform from text to numeric, but the text must be categorizable. Some times we need to create our own categories if we want to keep the data. As a sample, colors of a car, can be a lot of them, impossible to categorize, but we can distribute it in: Dark colors and clear colors, or maybe metallic colors. But it needs a lot of manual revisi贸n.  \n\n\nFirst we need to know with colums are not numeric. ","metadata":{}},{"cell_type":"code","source":"df_house_clean.loc[:, df_house_clean.dtypes == object]","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:05.395116Z","iopub.execute_input":"2022-09-08T20:12:05.395632Z","iopub.status.idle":"2022-09-08T20:12:05.442485Z","shell.execute_reply.started":"2022-09-08T20:12:05.395562Z","shell.execute_reply":"2022-09-08T20:12:05.441286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"object_columns = list(df_house_clean.select_dtypes(['object']).columns)\nobject_columns","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:05.444385Z","iopub.execute_input":"2022-09-08T20:12:05.445164Z","iopub.status.idle":"2022-09-08T20:12:05.457467Z","shell.execute_reply.started":"2022-09-08T20:12:05.445115Z","shell.execute_reply":"2022-09-08T20:12:05.456278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have 40 columns to transform, if we want to categorize all of them and we have 5 categories per column we will add 160 columns to the model.  \n\n(40 * 5 = 200) - 40 = 160. \n\nIt seems a lot, but it's not a problem. The problem is that not all of them are relevant, and we are fitting our model with a lot of useless informati贸n. But let's try it, we can delete it after all columns are converted when we can study the correlation of all columns. ","metadata":{}},{"cell_type":"markdown","source":"Just one sample with the column *LotShape*","metadata":{}},{"cell_type":"code","source":"df_housing['LotShape'].unique()","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:05.465407Z","iopub.execute_input":"2022-09-08T20:12:05.466212Z","iopub.status.idle":"2022-09-08T20:12:05.475165Z","shell.execute_reply.started":"2022-09-08T20:12:05.466159Z","shell.execute_reply":"2022-09-08T20:12:05.473873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ls = pd.get_dummies(df_house_clean['LotShape'], prefix='LotShape')\nls","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:05.477148Z","iopub.execute_input":"2022-09-08T20:12:05.47767Z","iopub.status.idle":"2022-09-08T20:12:05.49748Z","shell.execute_reply.started":"2022-09-08T20:12:05.477622Z","shell.execute_reply":"2022-09-08T20:12:05.496353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, we have a new column for each categorie, and  every registry have a value of *1* in the category where it pertains. \n\nThe next step is add this columns to our dataframe, and delete the original one, because we don't need it anymore. \n\nNow, we can do it for all the columns with data values in our Dataset, as we can see above in the list all of them are categorizables. ","metadata":{}},{"cell_type":"code","source":"df_house_clean.shape","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:05.499892Z","iopub.execute_input":"2022-09-08T20:12:05.500266Z","iopub.status.idle":"2022-09-08T20:12:05.508717Z","shell.execute_reply.started":"2022-09-08T20:12:05.500233Z","shell.execute_reply":"2022-09-08T20:12:05.50749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"     ","metadata":{}},{"cell_type":"code","source":"#Copy the dataframe. \ndf_house_cc = df_house_clean.copy(deep = True)\n\n#Create the dummies for each column in the list of Obcjet columns\nfor obj_col in object_columns: \n    #print(obj_col)\n    col_dum = pd.get_dummies(df_house_clean[obj_col], prefix=obj_col)\n    df_house_cc = pd.concat ([df_house_cc,  col_dum], axis=1)\n    \n    #remove the original columns from the new dataframe\n    df_house_cc.drop(obj_col, axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:05.510288Z","iopub.execute_input":"2022-09-08T20:12:05.511321Z","iopub.status.idle":"2022-09-08T20:12:05.858988Z","shell.execute_reply.started":"2022-09-08T20:12:05.511274Z","shell.execute_reply":"2022-09-08T20:12:05.857891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_house_cc.shape","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:05.861193Z","iopub.execute_input":"2022-09-08T20:12:05.861999Z","iopub.status.idle":"2022-09-08T20:12:05.86979Z","shell.execute_reply.started":"2022-09-08T20:12:05.861948Z","shell.execute_reply":"2022-09-08T20:12:05.868461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_house_cc.isna().sum().sort_values(ascending=False).head(5)","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:05.871757Z","iopub.execute_input":"2022-09-08T20:12:05.87229Z","iopub.status.idle":"2022-09-08T20:12:05.893516Z","shell.execute_reply.started":"2022-09-08T20:12:05.872245Z","shell.execute_reply":"2022-09-08T20:12:05.892304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create a duplicate of the dataframe witout the Data Scaled. \ndf_not_scaled = df_house_cc.copy(deep = True)\n\ndf_not_scaled.head()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-09-08T20:12:05.895267Z","iopub.execute_input":"2022-09-08T20:12:05.895659Z","iopub.status.idle":"2022-09-08T20:12:06.023068Z","shell.execute_reply.started":"2022-09-08T20:12:05.895615Z","shell.execute_reply":"2022-09-08T20:12:06.021914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Scaling the numeric data <a id='scaling'></a>\nWe have our dataframe without nulls. But we need to do some more transformations in order to improve the data, and do it more usable. \n\nNow we are going to scalete, if necessary, the Data. \n\nWe can do it in many ways, but we are using SciKitLearn, and we have two classes to do it: \n\n- **StandardScaler**: Will transform the numerical fields so that the mean of the field is 0 and the standard deviation is 1. \n- **MinMaxScaler**: scale the columns so that the minimum value of each column is 0 and the maximum value is 1. \n\nWith this DataSet I think that StandardScaler is going to work better than MinMaxScaler, but feel free to check it by yourself. ","metadata":{}},{"cell_type":"code","source":"#We can add this line to the import Libraries section. \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom scipy import stats\n\nscaler = StandardScaler()","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:06.024468Z","iopub.execute_input":"2022-09-08T20:12:06.025066Z","iopub.status.idle":"2022-09-08T20:12:06.096347Z","shell.execute_reply.started":"2022-09-08T20:12:06.02503Z","shell.execute_reply":"2022-09-08T20:12:06.095435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The easy way to scale: \n\n* scaler = StandardScaler() *\n\n* df_to_scale_tmp = scaler.fit_transform(df_house_clean) *\n\n* df_house_clean = pd.DataFrame(df_to_scale_tmp, columns=df_to_scale.columns) *\n\nBut in this way we scalate all the values un the dataset. I'm doing the same, but selecting first with columns I want to scale and copy it to a temporaru Dataset. We can select the columns manually, but I decided to Sacalate all the columsn with a std greater than 1. But this value is in a constant and I can do a lot of test changing this limit. ","metadata":{}},{"cell_type":"markdown","source":"The SalePrice deserves a special treatment. Is our label variable, and we must assure that it is following a normal distribution. ","metadata":{}},{"cell_type":"code","source":"sns.displot(df_house_cc[:TRAIN_ELEMENTS_INDEX:]['SalePrice']);\nfig = plt.figure()\nres = stats.probplot(df_house_cc[:TRAIN_ELEMENTS_INDEX:]['SalePrice'], plot=plt)","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:06.098197Z","iopub.execute_input":"2022-09-08T20:12:06.099003Z","iopub.status.idle":"2022-09-08T20:12:06.750012Z","shell.execute_reply.started":"2022-09-08T20:12:06.098958Z","shell.execute_reply":"2022-09-08T20:12:06.748808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#applying log transformation\ndf_house_cc['SalePrice'] = np.log(df_house_cc['SalePrice'])","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:06.751727Z","iopub.execute_input":"2022-09-08T20:12:06.75209Z","iopub.status.idle":"2022-09-08T20:12:06.758009Z","shell.execute_reply.started":"2022-09-08T20:12:06.752058Z","shell.execute_reply":"2022-09-08T20:12:06.756816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.displot(df_house_cc[:TRAIN_ELEMENTS_INDEX:]['SalePrice']);\nfig = plt.figure()\nres = stats.probplot(df_house_cc[:TRAIN_ELEMENTS_INDEX:]['SalePrice'], plot=plt)","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:06.759917Z","iopub.execute_input":"2022-09-08T20:12:06.760575Z","iopub.status.idle":"2022-09-08T20:12:07.404006Z","shell.execute_reply.started":"2022-09-08T20:12:06.76054Z","shell.execute_reply":"2022-09-08T20:12:07.402807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that our Label is Normalized we can attack all the other columns. Sometimes is better to scale all of them, sometimes only some of them. Thas why I prepared this code where I can select the level of std desviation that I want to use as a limit to select the variables to Scale. \n\nAnd I can do experiments changing this value and check how it affetct to the final predictions. ","metadata":{}},{"cell_type":"code","source":"#We need to decide wich columns we want to scalate\n#all of them with and std greater than 1 \nLIMIT_TO_SCALE = 1\ncolumns_to_scale = df_house_cc.columns.where(df_house_cc.std()  > 1)\n\ndf_to_scale = pd.DataFrame()\n\n#We have all the columns in columns_to_scale\n#we can copy each column in a new dataframe. \nfor col_to_scale in columns_to_scale.dropna():\n    df_to_scale[col_to_scale] = df_house_cc[col_to_scale]\n    df_house_cc.drop(col_to_scale, axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:07.40549Z","iopub.execute_input":"2022-09-08T20:12:07.405969Z","iopub.status.idle":"2022-09-08T20:12:07.456888Z","shell.execute_reply.started":"2022-09-08T20:12:07.405922Z","shell.execute_reply":"2022-09-08T20:12:07.456011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_house_cc.shape, df_to_scale.shape","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:07.458154Z","iopub.execute_input":"2022-09-08T20:12:07.459291Z","iopub.status.idle":"2022-09-08T20:12:07.472018Z","shell.execute_reply.started":"2022-09-08T20:12:07.459248Z","shell.execute_reply":"2022-09-08T20:12:07.470702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have two dataframes: \n- df_to_scale: contains all the columns that we want to scale. \n- df_house_cc: is the original dataframe with the data Clean and Converted to numeric. We don't want to normalize all this dataframe because the categorical columns. \n\nNow it's time to normalize df_to_scale and replace the values in df_house_cc.","metadata":{}},{"cell_type":"code","source":"#Let's see the data in df_to_scale befote to scale. \ndf_to_scale.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:07.473823Z","iopub.execute_input":"2022-09-08T20:12:07.474368Z","iopub.status.idle":"2022-09-08T20:12:07.50741Z","shell.execute_reply.started":"2022-09-08T20:12:07.474336Z","shell.execute_reply":"2022-09-08T20:12:07.506278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Scalate the Data is really easy, now that we have the all the Data that we need to Scalate in a Dataframe, we only need 3 lines of code: ","metadata":{}},{"cell_type":"code","source":"#Create the Scaler\nscaler = StandardScaler()\n\n#Fit the Scaler with the data. \ndf_to_scale_tmp = scaler.fit_transform(df_to_scale)\n\n#Convert to dataframe the value returned by fit_transfom function. \ndf_to_scale = pd.DataFrame(df_to_scale_tmp, columns=df_to_scale.columns)","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:07.509105Z","iopub.execute_input":"2022-09-08T20:12:07.50943Z","iopub.status.idle":"2022-09-08T20:12:07.522437Z","shell.execute_reply.started":"2022-09-08T20:12:07.5094Z","shell.execute_reply":"2022-09-08T20:12:07.521114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#After scale. \ndf_to_scale.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:07.525523Z","iopub.execute_input":"2022-09-08T20:12:07.525887Z","iopub.status.idle":"2022-09-08T20:12:07.54834Z","shell.execute_reply.started":"2022-09-08T20:12:07.525857Z","shell.execute_reply":"2022-09-08T20:12:07.547233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Replace the values not scaled by the scaled versi贸n. \nfor col_to_scale in columns_to_scale.dropna():\n    df_house_cc[col_to_scale] = df_to_scale[col_to_scale]","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:07.550046Z","iopub.execute_input":"2022-09-08T20:12:07.550721Z","iopub.status.idle":"2022-09-08T20:12:07.567403Z","shell.execute_reply.started":"2022-09-08T20:12:07.550676Z","shell.execute_reply":"2022-09-08T20:12:07.566369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_house_cc.shape","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:07.568927Z","iopub.execute_input":"2022-09-08T20:12:07.569495Z","iopub.status.idle":"2022-09-08T20:12:07.575538Z","shell.execute_reply.started":"2022-09-08T20:12:07.569463Z","shell.execute_reply":"2022-09-08T20:12:07.574649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets see the data scaled jointly with all the other fields. \ndf_house_cc.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:07.576983Z","iopub.execute_input":"2022-09-08T20:12:07.578281Z","iopub.status.idle":"2022-09-08T20:12:07.70955Z","shell.execute_reply.started":"2022-09-08T20:12:07.57824Z","shell.execute_reply":"2022-09-08T20:12:07.708216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Select correlated features in a new dataset <a id='correlated'></a>\nNom that we have all the columns in numeric we can select the ones more correlated. But we are going to keep the dataset transformed but with all the columns to test it with the models. ","metadata":{}},{"cell_type":"code","source":"list_columns_ordered = df_house_cc.corr()['SalePrice'].sort_values(ascending=False).index\nn=0\n\n#I create a new DataFrame to contain only the columns that are correlated. \ndf_correlated_columns = pd.DataFrame()\n\n#Minimun correlation that we want. \nMIN_CORR = 0.4\nfor col in df_house_cc[:TRAIN_ELEMENTS_INDEX:].corr()['SalePrice'].sort_values(ascending=False):\n    if (col > MIN_CORR or col < MIN_CORR * -1):\n        print (list_columns_ordered[n])\n        print (col)\n        df_correlated_columns[list_columns_ordered[n]] = df_house_cc[list_columns_ordered[n]]\n    \n    n = n+1\n","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:07.711187Z","iopub.execute_input":"2022-09-08T20:12:07.711639Z","iopub.status.idle":"2022-09-08T20:12:08.59785Z","shell.execute_reply.started":"2022-09-08T20:12:07.711579Z","shell.execute_reply":"2022-09-08T20:12:08.596699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_correlated_columns.head()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-09-08T20:12:08.601463Z","iopub.execute_input":"2022-09-08T20:12:08.602332Z","iopub.status.idle":"2022-09-08T20:12:08.634901Z","shell.execute_reply.started":"2022-09-08T20:12:08.602293Z","shell.execute_reply":"2022-09-08T20:12:08.633676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ok! **OUR DATA IS READY!** Let's use it! \n\nI have 3 different dataframes to test: \n- **df_correlated_columns**: A dataframe with all the modifications, and only with the columns witha correlation > 5. \n\n- **df_house_cc**: Without nulls and with all data in numeric and standarized, but with all the columns, not only the more correlated. \n\n- **df_not_scaled**: Without nuls and with all data in numeric but not standarized. \n\nAnd their respective test datasets. \n","metadata":{}},{"cell_type":"markdown","source":"Just, one last step: Split the data. ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n#Split the data only correlated columns. \ny = df_correlated_columns[:TRAIN_ELEMENTS_INDEX:]['SalePrice']\nX = df_correlated_columns[:TRAIN_ELEMENTS_INDEX:].drop(['SalePrice'], axis=1)\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\nX_test = df_correlated_columns.loc[TRAIN_ELEMENTS_INDEX + 1:, :].reset_index(drop=True).copy()\nX_test.drop('SalePrice', axis=1, inplace=True)\n\n#Split data with all transformations and all columns. \ny_cc = df_house_cc[:TRAIN_ELEMENTS_INDEX:]['SalePrice']\nX_cc = df_house_cc[:TRAIN_ELEMENTS_INDEX:].drop(['SalePrice'], axis=1)\nX_cc_train, X_cc_val, y_cc_train, y_cc_val = train_test_split(X_cc, y_cc, test_size=0.2, random_state=42)\nX_cc_test = df_house_cc.loc[TRAIN_ELEMENTS_INDEX + 1:, :].reset_index(drop=True).copy()\nX_cc_test.drop('SalePrice', axis=1, inplace=True)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:08.63675Z","iopub.execute_input":"2022-09-08T20:12:08.637095Z","iopub.status.idle":"2022-09-08T20:12:08.720156Z","shell.execute_reply.started":"2022-09-08T20:12:08.637065Z","shell.execute_reply":"2022-09-08T20:12:08.719131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_cc_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:08.721945Z","iopub.execute_input":"2022-09-08T20:12:08.722778Z","iopub.status.idle":"2022-09-08T20:12:08.730393Z","shell.execute_reply.started":"2022-09-08T20:12:08.722732Z","shell.execute_reply":"2022-09-08T20:12:08.729198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The Model & Test different DataFrames. <a id='training'></a>\nI'm going to keep this part simple. It's a lot that you can do if you want to copy this notebook and improve the results obtained. You can test new models, I only test the ridge. Tune the hyperparameters, or maybe try blended models. ","metadata":{}},{"cell_type":"code","source":"def evaluate_regression(y_true, y_preds):\n    from sklearn.metrics import r2_score\n    \"\"\"\n    Evaluar modelo de regresion\n    \"\"\"\n    r2_score = r2_score(y_true, y_preds)\n    rmse=np.sqrt(mean_squared_error(y_true,y_preds))\n    mae=mean_absolute_error(y_true, y_preds)\n\n    print(f\"KPIs-------------------------------------\")\n    print(f\"r2: {r2_score * 100:.2f}\")\n    print(f\"RMSE: {rmse}\")\n    print(f\"MAE: {mae}\")\n    print(f\"KPIs-------------------------------------\")\n    return ","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:08.732464Z","iopub.execute_input":"2022-09-08T20:12:08.733307Z","iopub.status.idle":"2022-09-08T20:12:08.741693Z","shell.execute_reply.started":"2022-09-08T20:12:08.733259Z","shell.execute_reply":"2022-09-08T20:12:08.740549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import linear_model\n#from sklearn import \nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_absolute_error,r2_score,mean_squared_error\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\n\n","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:08.743566Z","iopub.execute_input":"2022-09-08T20:12:08.744052Z","iopub.status.idle":"2022-09-08T20:12:08.842609Z","shell.execute_reply.started":"2022-09-08T20:12:08.744008Z","shell.execute_reply":"2022-09-08T20:12:08.841725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_eval_ridge(X_train, y_train, X_val, y_val):  \n    model = linear_model.Ridge()\n    model.fit(X_train, y_train)\n    print(model.score(X_val, y_val))\n    y_preds = model.predict(X_val)\n    evaluate_regression(y_val, y_preds)\n    return model ","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:08.845096Z","iopub.execute_input":"2022-09-08T20:12:08.845579Z","iopub.status.idle":"2022-09-08T20:12:08.851824Z","shell.execute_reply.started":"2022-09-08T20:12:08.845535Z","shell.execute_reply":"2022-09-08T20:12:08.850543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1 = train_eval_ridge(X_train, y_train, X_val, y_val)","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:08.85337Z","iopub.execute_input":"2022-09-08T20:12:08.854541Z","iopub.status.idle":"2022-09-08T20:12:08.903918Z","shell.execute_reply.started":"2022-09-08T20:12:08.85449Z","shell.execute_reply":"2022-09-08T20:12:08.902492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model2 = train_eval_ridge(X_cc_train, y_cc_train, X_cc_val, y_cc_val)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-09-08T20:12:08.905728Z","iopub.execute_input":"2022-09-08T20:12:08.90618Z","iopub.status.idle":"2022-09-08T20:12:09.073973Z","shell.execute_reply.started":"2022-09-08T20:12:08.906133Z","shell.execute_reply":"2022-09-08T20:12:09.070874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test_features = df_test_cc[:]\n#print (test_features)\nresults1 = model1.predict(X_test)\nresults2 = model2.predict(X_cc_test)","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:09.097466Z","iopub.execute_input":"2022-09-08T20:12:09.100623Z","iopub.status.idle":"2022-09-08T20:12:09.14506Z","shell.execute_reply.started":"2022-09-08T20:12:09.100533Z","shell.execute_reply":"2022-09-08T20:12:09.142848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission <a id='submission'></a>","metadata":{}},{"cell_type":"code","source":"submission = pd.DataFrame()\n\nsubmission['Id'] = df_test['Id']\nsubmission['SalePrice'] = np.exp(results2)\n\n\nsubmission.to_csv('./submission.csv', index=False, header=True)","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:09.14839Z","iopub.execute_input":"2022-09-08T20:12:09.149341Z","iopub.status.idle":"2022-09-08T20:12:09.185255Z","shell.execute_reply.started":"2022-09-08T20:12:09.149289Z","shell.execute_reply":"2022-09-08T20:12:09.183175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission","metadata":{"execution":{"iopub.status.busy":"2022-09-08T20:12:09.188456Z","iopub.execute_input":"2022-09-08T20:12:09.190354Z","iopub.status.idle":"2022-09-08T20:12:09.239893Z","shell.execute_reply.started":"2022-09-08T20:12:09.190298Z","shell.execute_reply":"2022-09-08T20:12:09.237883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inspirations: <a id='inspirations'></a>\n\nhttps://www.kaggle.com/code/serigne/stacked-regressions-top-4-on-leaderboard\nhttps://www.kaggle.com/code/apapiu/regularized-linear-models/notebook\nhttps://www.kaggle.com/code/modassirafzal/housing-top-3\nhttps://www.kaggle.com/code/lavanyashukla01/how-i-made-top-0-3-on-a-kaggle-competition\nhttps://www.kaggle.com/code/pmarcelino/comprehensive-data-exploration-with-python/notebook\n","metadata":{}},{"cell_type":"markdown","source":"# # Feel free to copy and fork this notebook. \nI'm sure that you can improve easily the result of this notebook just working with the Model, or maybe changing the number of colums to scalate, the limit for the skew, or the limit to obtain the correlated columns. Or maybe using Mutual Information instead of correlation. \n\nCreate new features, and delete some of the used... you can test a lot of ideas for your own! \n***\nJust, if you like it, **please consider to upvote**! \nPlease, if you improve the results of the predictions using any technique described, consider to share your notebook in the comments i will be delighted to explore and discuss your solution, and of course upvote it! \n***\n\nThanks! \n\nMay the Data be with you ;-) ","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}